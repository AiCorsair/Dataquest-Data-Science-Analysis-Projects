{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73901a6a",
   "metadata": {},
   "source": [
    "# Building a Spam Filter Using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212de33",
   "metadata": {},
   "source": [
    "## 1. Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a89486",
   "metadata": {},
   "source": [
    "In this project, we will build a spam filter for SMS messages using the Multinomial Naive Bayes algorithm. Our goal is to write a program that classifies new messages with an accuracy greater than `80%`, meaning that more than `80%` of the new messages will be correctly classified as either spam or ham (non-spam).\n",
    "\n",
    "To train the algorithm, we'll use a dataset of `5,572` SMS messages that have been manually classified. The dataset was compiled by `Tiago A. Almeida` and `José María Gómez Hidalgo`, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/228/sms+spam+collection).\n",
    "\n",
    "To get started, we'll import the necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93490cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the relevant libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the SMS spam dataset, using tab as the separator and specifying column names\n",
    "sms_df = pd.read_csv('Datasets/sms_spam_collection', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "\n",
    "# Print the shape of the dataset and display the first few rows\n",
    "print(sms_df.shape)\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87c481",
   "metadata": {},
   "source": [
    "The two columns of the dataset represent the label (`spam` or `ham`) and the actual message content. The messages vary in length and tone, with `ham` being the most common class in the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c234a4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the proportion of each label in the dataset\n",
    "sms_df['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4c739",
   "metadata": {},
   "source": [
    "We see that about `86.59%` of the messages are classified as ham, while the remaining `13.41%` are classified as spam. Since most messages people receive in real life are non-spam, this sample appears to be representative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef70ac5",
   "metadata": {},
   "source": [
    "## 2. Creating Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da825c03",
   "metadata": {},
   "source": [
    "When developing software like a spam filter, a good rule of thumb is to design the test before creating the software. If we write the software first, there's a temptation to create a biased test to ensure the software passes. To evaluate how well our spam filter classifies new messages, we'll split our dataset into two parts.\n",
    "\n",
    "We will keep `80%` of our dataset for training and `20%` for testing. Our goal is to train the algorithm on as much data as possible while still retaining enough data for testing. With `5,572` messages in total, this means the training set will contain `4,458` messages, while the test set will contain `1,114` messages.\n",
    "\n",
    "When the spam filter is ready, we will treat all `1,114` messages in our test set as new and have the filter classify them. Once we obtain the results, we can compare the algorithm's classifications with the human-assigned labels, allowing us to evaluate how effective the spam filter truly is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a9b906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 2)\n",
      "(1114, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     0.866981\n",
       "spam    0.133019\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     0.861759\n",
       "spam    0.138241\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomize the dataset to ensure proper distribution of spam and ham\n",
    "sms_df_randomized = sms_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Calculate the index that separates the training and test sets\n",
    "split_index = round(len(sms_df_randomized) * 0.8)\n",
    "\n",
    "# Split the randomized data into a training set (80%) and a test set (20%)\n",
    "training_set = sms_df_randomized[:split_index].reset_index(drop=True)\n",
    "test_set = sms_df_randomized[split_index:].reset_index(drop=True)\n",
    "\n",
    "# Print the shapes of the training and test sets\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)\n",
    "\n",
    "# Display the proportion of each label in the training and test sets\n",
    "display(training_set['Label'].value_counts(normalize=True))\n",
    "display(test_set['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc9fae",
   "metadata": {},
   "source": [
    "We observe that the percentages of spam and ham messages in both the training and test sets closely match those in the full dataset, where approximately `87%` of the messages are ham and the remaining `13%` are spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ae6c7",
   "metadata": {},
   "source": [
    "## 3. Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94423b9",
   "metadata": {},
   "source": [
    "### 3.1. Explaining the Naive Bayes Algorithm"
   ]
  },
  {
   "attachments": {
    "download.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAFECAMAAAAqbXvXAAAAAXNSR0IB2cksfwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAJlQTFRF////r6+vf39/j4+PYGBgQEBAcHBwX19fAAAAICAg39/fEBAQn5+fUFBQv7+/z8/PgICA7+/voKCgMDAwT09Pb29v4ODgkJCQPz8/8vLyl5eXiIiI4uLiPDw8Hh4etbW1Dw8PLS0t09PTxMTEWlpaeXl5W1tbTExM1NTUampqPT094+PjxcXFLi4upqamHx8fS0tLwMDA0NDQrwhRrAAAJjVJREFUeJztnXlj2r6y970JJHnDx3X7JPkFSFLSLLc95973/+KemZG8gTHE2DXJmc8fqbG1jr4aSaZIjjMzrud33PNmKAnzyQnEYv/WUqqDYEsp/0px/jvRCyVDEcHVQojA3IuFwA6dBPAo0HOW7gKUFPu3WF1/mTSUxMpxBPyT4b0ELkBn/zJP5L/mLuMwWF3zk0vp60ShrlBdNAcJjLqUVAk+CrO5CzmIhrqSRRCgdwZ1pXCV2XsLc4/VNRlSfoO/rpQRqYsaRJG6tNGaL2U6bxEHUqtrQS44QCWF6KoVyGtF9xasrkkB11ROrISMQ5mg1JSAtshCqT7rnAup1AUDfZEUEqoGShLREnsNdJ0crxJW15T46LAiUhFISqC7CvDfwDyKo885LDoNdekIKqHRO4PvcnAyIKCO2KlyqCara0oiZSf1oC6RgPnBZ6UK1eV4+Eit5i7hQOqRMVvFIjfqwlk9akzAYjgIMAira1qi2Ey4QF2grCSCJjDqgl4vrPI+IbXvgtE/jpvqkjS1lFRtVtfUaIEzECFzp5BFLP1SXQ41jfycg2OlLphy6XJkRHXFUM8Y5pQEq2tCIildp1ozKpgBh9gWuMLypPxuQiRzl3IQSuakn2yJ71Qioy5Y/2pcJnvkkpOA14xTksHaSdP63CV14ZAhHFIXLLUC7WTx530jYYhAWCqnd3lLnEdKrFAGN0JlFTd3Sb8u0InpJZByjLo8tLhj3w6ZtghOp3KNVOqiOhYhrQ8FDJMhfpWtsXZ4xeqakgi7dRhkVl2apllGUrScDL3POe1qkums41p/5rd5n4bsuJl7HjEMwzAMwzAMwzAMwzAMMwIe87WYW1At5jYGMzJzC4phGIZhGIZhGIZhGIZhGIZhGIb5byIVocyjqXPRgZI5bb8YCAP+uDOp8jZ3aZeXRRkgESWmeBFdx/ghqy8d3HizjCO+O2UsDwJV1cpW5e6PiXAbxYrtrwB9iAhlKHeDdUVchqAiUsw6CyxKaPaZdBoFKWv2PaoK7trUq0ApXQYu1cGr44iyemVl4Ym2kdsVcMlyTkAlT0ViAojr/JZRmz0TD3fZHTmXsIiWCneaUritCZDib4uF7y/pR3l0V9BWVKIMkNAtAX9NI3kyph1RAvw9X15eOvirZN9EMT8Wj+k6gkClybWSReQp/DF8JBs9KSp/wbzEn8sr2tkHyau9FV2pCigi7lQmlckkw43MoOCCNgahu0LmcFeVAVz4k1MxEpt6QKXFX61jfWK61Fj6gmLYX8Caglv1lxssLPFnolkuhbdU+Hv/hNrK/KAR4qQmwJX+WDaQKknzqfdFFBK7nQ4FtkF5M5XkIQq0mbkbyQLCNnfNbIrBox0TIHwlnAL1Um+NuELDR9WGCrW6AhKRDtWeumIVmiBWXTZqKsOyDAp1BS3qNfOJTMJLyj0wRYM7qrkDqNfY12Fpfkccw63EZC9op4RKEQGawFavRNkuT5EXRlIKmikzP5GlH2NDT3SuWl0hNmjSu8eFtvtZJl62Mle4c0SwID9t70GYRc9PNq2k4ryproAkB+b6Vt3Ff06pK6P9HKl901ara2v3A3WV7Yg9vZmglv7SuCujrjwUtmBVGWhHSRz/murKzdMETWeSNnU4oS7UqFWXSz2kVAR1qgN1hYq6PEbObDHIb1HZ4lxhnPDbVauLtrTBv8eHxiQMwXUvsPoqF4pCgv1jGmiW5p4X5qJvnwxRP6vVZdvScSvf5YT5aXU5R9SV0cDbpS5XmtFGw7yumWAkdWJCG3UJj4abLAyWZRnyakfJWl1lWztR5bucc9SVHlGXNg5yX10qId+OkZOyeVBNBYYLiyLEFP1PoC5HyuOzQoF1L0Ae5P7JO7tYKxrnluaK/lIP7CYNpZ2DoNUQXfkFexf//kAjiSqA06muCDLT5bbHjZFxYYLCY4rfUFezpZsJohyMxK26SK2oT1HPu8Ky4DKndLPGiOuUzbrCVFXeKHiHujCQUVcmaK8qqwjrwz3pYvSyG4JFaM6AkX1ZGc+hJKBTJFBkHzvt51ZXjg5ag1VN62K3y8wOnqo0XE7jS6vv7uezlDD/xWiq3POkZRGYAHkLMzkWJoApUFtdP6IoWuDUX8MkF8NjIKuuyCYXmejqtLqopY27MupSNI/Bga/2n7glm1ppshGR7Kkrp4KIVs2cfXVBuaNYkjRizwtC6qe2wD9scl65z561iAJvDKZHC3tNdWXQiwvc2tJ34tD57OqKQBZu7brJtpkfCxFW6jKN0acuHJbAvCkayOwGtqcuqZSS/y+jxMoAzr66SDaFxkKESoXmmWkPrVTp7VyzXR0EisqIneoKUFg0Ya/U5ULIFLdga4zOuHEkpi2FLVdbXVSQhal/o+BtdVHBPcqOwrtOpS5driAghi24sQhpcbHnu2Dm4OS5I77RXJAsTk2grlhdWZ+6HF3ktAdcrS4YHosoUh9Sl4PjY2vNiJOsErorwsw5Y95lC4EODK/N36BsgOa8Kypv1e+46gSzMMTXS9K0kFFXBiUMoBStMmDrBq15V6Mp8ToLD+vfMTIi5C+Noq26lNQH1assAotijFwVmsZxmKXgZz+kaZfjoQO7TnWBrgqcHUq3N5jG1bSpPlapIIuIs9WV2teKQjbVJexrEFlqzlj9bHXZFqRWj6ruUasri2y7pfYhuoA6QftiTJjFiVEXJKnDoC5DIkxi+KheM9oq6GrN6FMzn60uqLku1bWqCt6lLtBtTGvG2FYEp7au9CTFX1EeKW16eZ37WpLXD6Q8+jpBKzrqiDaHwxnngvamz2iNdq66MtsyGKNWV2QGFGpvc5esfr66MurKmLZupnpoaGFWfqr1vsvmg9OYWl2ZzLGWlbpMESmjWl2eWcKtsEtaB4Qd5Xx10WpQm9fCjVQP1YVbpWJku+aOqZkyaZYiMMQe1PTKSMyWiceXe9A0UZrkUC8PT6RZSWqiOHHphfaZI6MnAzdxaRauwojI6B12kqxoWDG2TGjNaAOQIHrVhcss0+qx9MpIkSzoCmbfcfmaJQ1VhNnTy9bABKyWbIKmzt/L0VnmTkPhVESoKi5CVFnwLJeLxF2RQzGpkNdVVQDnhLroRaw2p/x4ZSSzaMGCE+UrQlIXTEX8JBLWz6nyXTIZ/IeADOMr3VDcVbgvYk8As3clvUwucDdYbPUC97iMZXb2vMtsv0jjR7X2wimD3a6xtKWgLbMba69+dcHAkZmBVVaRomqP87TuMxo3hsxdp1pSyqgoX8FFtOnr92p0jpym/6SC52blY4CQ2bLaTNJqFCXarNkJdeHbHFJXmShNPGR5Uo5TWyQLZd0ItrcUJm0zkSCHljXnsNeF1ie+BrI7J2Lrlkua7FScY4l8NO9LaCY9fPfHYwUfmNxQjleA3gB/+m1T9yYGDDMirC5mOtzg07tfhmEYhjnODfO1mFtQLeY2BjMycwuqxdzGYEZmbkG1mNsYzMjMLagWcxuDGZm5BdVibmMwIzO3oFrMbQxmZOYWVIu5jcGMzNyCavHx4t/ejW8SZjTmFlSLqlT/3N6fV/z79UR2+ats7rfy4UQ/WZ9pkavgESp0/3Sd6tr83Ep5XjW+hLp2cn139ywfegP9+tXz8G79Mm6RmryeY+NfTfE/rrevu/X28RrVtaP/D3lezb+Cuh7l2zv8cyeHj/I7OaG6ns9pi1ZD3Mn/AR8B9ZlbUC1M2Xa/d7/OV9c/u5+vj3gJFz9v4d/33WbzE+/dms9Xz7ORxvsWu39Zi5vb28dXrMXm1dTvDu7+ufvnzj7Gaj9tdpTA4+5evqI0MexTmSwExjA3TyaBO9Lu7e7dPMWgf8wVpPhokjFp/9lBzhiWPj7tHuTO2vHuz+NraW0sCHWKP5uftxvwVLs/T7snSmbzssEg8vUa1fVO7uu8lrnfrn+/SQm1etpun98kNNCLXG/f3uT6dfv2W36GycqD7fZ//tCYArV4eMd+s4WabXcS/q7h8/oNdfjw+20tKRzdN1Z6gnu/n9Fq/35ey51N9tmY5hUDwiD1KrHJ12VeVdBn+faLMjAG3OGd9fbfUADz4A8k/vZqYq1NmR5NOU209YP89+vt21a+3W0khrujjECc8vYa1XXzEXXhcEK1esCqoglfpLnCv89TjhhjsX2rr9c4gbrFNr43owv+/YOVNOqCBy/o457x1su2tBKNjI8Sp2bPtnFtEIlaRANtMO5TOfquf99gFu+QFyX0illbA1IukCRF3DVHxjWW5gm77K/tS1kukx+NjPfbm6qzPKKQ5xZUi8rI56uLqrIFsz6iw95Af3kh42zIjrelqa8ZWasLy39jWsnUjJ5RjYy68B4235qWAM8tdZmu9C6tozEPTUD5bKLdlb2NRmEcwIwWIMM/lPX7220ZkUr18NBSF937BaJ8f2yW68YWF93Wo8n/kfrJ3IJqUVn5g+qiGm5e7+8fGupCY30KdTV8lx1VUCzH1YVPUC776rIT67VdezZFImnc3Nw8vFUZrc20S25/AjDE7tAv1RHf5QM9WB+qi+bsT6//AWvvqQuVfWvS+Q/peG5BtaisPEBdOxj4b18/o7rW9XLLrv1Oquu9S11WVvdH1PUud4+yWua8PP+W61tU1xvRWHZSRJhf0P3nLnVtwNOBte/21YU+0GZPo/TXURfZe/MZ1fUszTpu+wDl/VPe6fdd2w51/dqaZN7KZPFvrS6Id9uahr48QDbb8jVabSrru15vmp9v6sReIRWatx2MjFh+O7O7JQ82t6BaVBX/uLpeyBq3n1FdZqlFc2Tjk/5BL9SvrvstKvK+pS4zrN6W/ulAXXeyGoOf1qV17nEFiJNWYzb0bjYXyuHxvaUu8kvoa8k1PR2o6327bgl4bkG1qEo1wHett7eb3fYzqgucwfMGyo7T4J3cPW0esNT96trIh82f18pKd/Juc/O+Xt8+3lbj7IG63rf1wAhBnza4Snzarv88/sGvmV7lHd56sRE3cr15vMPF5A4TN7Hk89PmF6YCa3W09mulrl/bzRNlagX8H3JhcwuqRWXvAep6WUuYSEB9P5+6bnZQdvlmplxbqAYWul9dN3cQ5e2ttNL7WsLNl19VMjcd6gJJVH7l5Q2D4uj1RHmjn3q1sa2v+gMPtq/vZFmr2PXvZwiD5n35DQ/vcOVp1bXZVlMTKpBxk3MLqsVlbUTvJD4pj4/v9eWZUd7N+6XqE/BeJ9PBffOryvcqnyrS+17WVaHKf0FJjcDtxM2DamZnQs0tqBZn2JQhHnHgeTQu6ew48tIvxtZvHwwxt6BaXFj5/yI22/XPn9uP/M+IDQS/NNdT6vp5L59aN+YWVItLa/9fxMvr269d3zC4z5/fvy7+Vux11//87e1P+8bcgmpxae2ZK2NuQbWY2xjMyMwtqBZzG4MZmbkF1WJuYzAjM7egWsxtDGZk5hYUwzAMwzAMwzAMw3yERITmHByGGZsf5uijxdzluEp87nUXocFxpWku5Rc5RsMe17kypxmLyFmo82rm1we0kTf/gVftU9ti5UzJyaN5+wji0coxIi4d+BbJjiMzK/QiCOjMSi9bmSu4DoIFnrnl2nsQZvG3T/3qQptT80I60ggPAvXz89RVn3K3krnvL8056q0GX17S/CXq6EmY4hKBqGmVPxCdoKz8nrNlnSQMA4FDpydVLhSdqxpIEStz5C/d88JcyPLAw1mhY1cTSaWMP+CRK3Vpc0pwQSfejqGnNnKaA6zV1Z7P6GRK9phR4NiCZ2V6dNQ5HtLrYuPhCZbQKHRFf/sOqP1rLPE03yIUWKFcoHN1nNTLosr9Rg3362Gf8twkiGp1Baaj0WHFpK7Mhxh4L/IpAb/01XVniqwndw6du5e4eAceLHCQ8GTumXja07YomZdEgUuTPM+gba5l+n6E8U0OeD+jOmHGLlVKT9EPxiLuc11Ojmc+6ySzJ+i50KkzCl6fup7TAc1XUUM8Wd3Jv/lQaG1OvKaDlkNBx/165god2wrdr4KKyFDCcFWpK7dewHVNjaA7BbHEHiZU5b8X4KvDajYRQ1KhSk1YIVdl6mFapq7B/2O2hZChPcg9kQqLgjlIJZVPmSkEpylZHsai6vNwE8YGOqodIsV4lHliOjQOiAFYX1yD7TtZmXHkGJFUgYsdxaiLDg3O/FiI8Pxzsf8eKdRFyyiFFnLx1GyrLhRHiFVY0nnHeM+z7leG2NyVuppuHGtEvoza0qjLq321HeRMQ4ffQJmoVjxX2KSOkyGpMHVl/H/WGBkTmdM9FzWindp8FDfAaUZ1bLPCiwQ7C91PsdghJBTR/FJ9u2J1gbj6jsUGKxWwplw11QU9qIgidY3qcqCVIxBEWJjWNOrCFsaPHs0N8Z4pMo6j8pvj9KiL/DR1KaMutIGQ5VPEJfslKSkbIoAKYnLmkciMmlySCamwoS4aFeGzPeTbJqdR+5kJFn6ry0FRraKxKgV6rCD0sC9dr7pOigvRMR0LjpZFExY0koqrVBc0bAwFKXJH4BhXqwuLb6qA92CoAnL4bBqM5mvIvrqcdBGLvE9dWS6Fj6qNpMBEIcG8SoRS9+hBjInsqQun42110WgNjg1TKs9Sr9SVlGfRu6DYNJNuIagvXau6TnsuRT0TqmU6/gKaaoleIbtO3+XL76GP7iKjVcYxdcH8h8hKdZVL+ubqC2vkgnQiv09dkEEc4ugWyZzS9BuPrLrMg+hQXWJPXQtJE3WpKEJQl4OiRvQUHrvg3nxXwlWGfelK1ZVJy/GFsgijNMmhg8CMNtErSZOWOHFzeZXqSiTNTTJp2umYuvba30ld+7F8jVGuGWmW3zsyElEoyrZ3cALWSt2vX9bsqwuiNdUVmWvdnglX6jJjr1m6gPa+wRzSN73fda4QfVpdegmPQxctW8B6RuCgWMCtAtvh+tTlhObNIqzHsKTH1GUGd60P3kAl5luxqHzfRSNl2qeugr4OgKeZGVVTDIapB9J6RpsmlqKhLjMmFE116XIsNP+WK/lKXeZ1nnl36odhhD73Kl4zXkCmqZpoWa3LW1f71ZEwDQijEf5zTF0weY60S/Kh4D9EWaGlDJJkFaJk8LGQPnzCUfaYuhLw5ODTA3wapOkKXygocvNFmfpSLtIU5xQgjCS1sXBMUKFuqktJLwJS8E6YZmgnLbW6fMghWZDb09KsZqkv/fj0XxQby149Xjk3se3arS5Hw0KYHLEJF9ev/Dzw0GFQviTAcKGPPuPoyBgpdP5ZGdW3qZM2rK9CZ4+uBr8WMZIBf4aBU6ehrnIk8Uyapbga6jL3/fpuFlIG9VD8Wfkk6jqbtv9tXuv2k9NfoNYRSud+4N33vT7Ou3RfyvrI8NAd6WqHknNxg09fhSsikfzfe5ipYHUx06GDnv/0xDAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzDMpwL3ZZi7DMxXhdXFTAeri5kOVhczHawuZjpYXcx0sLqY6WB1MdPB6mKmg9XFTAeri5kOVhczHawuZjpYXcx0sLqY6WB1MdPB6mKmg9XFTAeri5kOVhczHawuZjpYXcx0sLqY6WB1MdPB6mKmg9XFTAeri5kOVhczHawuZjpYXcx0sLqY6WB1MdPB6mKmg9XFTAeri5kOVhczHawuZjpYXcx0sLqY6WB1MdPB6mJGJVsJt/rQUlcSB9kcJWK+DgEI6l/lh6a6/gXX3jxlYr4KsWzIq6EuFJcUc5WK+RoksiGvWl0kLukej8cwZxA15FWpy4grmrNczJegIa9SXSwuZixqeVl1sbiY8ajkZdTF4mLGpJQXqYvFxYyLlVf1h8XFjIiRVwWLixmTiMXFTEfE4mKmI2JxMdMRsbiY6YhYXMx0RCpkcTEMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzAMwzDM38X1/I57f2+b7TT5aIykXbrswwmMR5bo+TK/jAmKHojF/q2lVAfBln/vNIr04z+M9lql03PuOC9kPlveFxJ0tPuFqMPd2adSl1KHPrGD5JOr6+wm8tXorblPrD6w9378qdV1bqMnHx7Y2upy9Iwbzmv37HNgvOnHgw9o/UNFP5eGupJFEKDXAHWlcJXZewtzr9sSZRy4CoIFjdvZCm5pnKpFaeDbzzDggTGF11F+z0sjk4jvJVHgZnDDiTzCt0U4IpcyK9NO+MmHT5BAAtl58BlKlJQF3I8L2ZTPMt8UPsX5W4a5as/rmoSUGZrqYi9I9vKJIDYkrV2btIbSa6zRAb6A7uY20+ywDKVKaWDhUirYfuerDViZAIu/CrSnZOhBCV0y7qEZXI8axsVq+Ga+XVvFXlGLucY+aDNItqus3dTqWtBGQwEqKQzhSkG6K7q3OKou8zzPzME7tIGyVnihtElGOGlInzOzkdH3wzTgIT7xsSwQ1tM4Mgqz7ZEqs1h15a5DmxWpKy0/0cgYmWTDha3UYWQoNz7zyiJD4RMsoCtl5vgy7MvQVndFwq7zWRhv0UiaCqU6x3plSlYVu8sylEyAIcIsC7EeUK99ddUGdMu0Err3v1RIT2BddJcVPHwSoAKE/GbGLJvnokoW5WFaw7UV+8DQUqkLClQkBcYFJYloicbB1PAqOaKuDOJoHysGlQ6iHGwACSofTF5gFJkH5nMIooEgcdThu8Bynq9MVBnmPqkLfZewRRCmCF1lt1mRuuCTC+b9VqmLii4LSFt2ZKvLfEkqtgQhZB1g/jGk05MhVTfGgF6Zj2fyseoKfZO0sg861OVi/ERjmn7YeTaRLaHJwHdIJR1jXdOAyo1QhHhQjcr/L4K7Ueqj9aDEh/6TehMVO4T0rbrKxOo6QQcvMKlMo8zFEHXpCJqeWnZJ/TaHB4EMNV4FR9SVUo+JosQkk6G5yJIFFBSi4PhIwihMwTvnXXSboinyl9q2hSaTB1hRKkKXbWxWpK4oSivX4Vlroti6DWvn/mD575q6amoasEBrB5BhxwqkzjCnNsYCeq18vpcF8DGczFJK2u1ep5hSk328ziJSCQsyozSSSbIOF1QbkEyQw6o1sbYmLYLTK7pnYPDETXHzMo35L6tGQqvA1TdbPIV5Qji30wH2UY+M2SoWuVEXFgQ1Br0lCAIFxT0yMmLHClzSVQ4hoWh1U5pkwG7wIMfoferK8C/VolKXouEit0Xo6Nt1Vmbe5QZChA11fXdMih2DiVOqC4Ml5jmG9WWeyDwMs87WrjNE/2VM5LXyqdQVUaG+m751ZBVMpS6o6LozBJXQ1A2bFVXidhSsNmBlAlslqyn4m5kS7wO9yZexDFx0KLW6IqOugIqe6LJx/Q+vxmvfBa4vjpvqkmZqQNOfI+rSSzND0+Xmo15Uza2qZAz96nIO1LUo29zQrS6bFbXAAo0wRF1Rra4ExgUZCbnqnHbVGZoyo1H61WVi9Klreb660KJd76RqA1YmaKsLPvnkAQ/wpRBSh2GBfqpHXbZxh6uLPLCu1RWDx4rBfRA9byQSmqFBUShgZppKJ4lNBqqVmiR61UXjUlNdPyrfntsiHGBcCWaFLWBqLgaoy76+pd4dwsICZpKqc9pVZ0gTbDLRSd/l9qvLp0lh0nls3766IFTY4YIqA5qJijhQV4a16nzJC72JXAfON4+qC/zewjTuAHWZ1suWOMGJjLrAiBqHXY+WRUlwbM3o07zMDOshGP7HCmuycLKYBlPbbwKYz8V27tAB+aVVNb6bttHlPNcW4eALBYcGA5sVtgCZNlMD1AUVyDWWwKVTQXNMuGva1chwiRVPsUL96gJr5DQrPqIuWt1Rml0Lj311OThQHHazyoCkYjJBqS77gnR5dGPY0K5RsNscUxc2LuS68j/+nroceSJcwue0jF6aRTRkmMGNUFnFdUQG46lAYLPgulqFWHfwgfhCo3SB+BlTE5RV2NHM0kRQTlNdeTmeZsoUoeuNV5UVtgCURQpK58Pq8k0WKGefPJjonHY1MsRXE6p8GdKjLvxDr3e62hbCh/gegEJ0tdqBugrZ5YIqA9YmKNWFBfbNWafdb6kEBsykncQcURdO/LFxh/guqy6shixCWh8KWyqYV0lzdWzeJWQZEq9yrJIXllHMFMFDfeBJ5q7qbGYpY2xa3VJXWaxGETooszLvuzCVZWmDj6jLibCIBXqP1C7hjrziLjN0NMpfaOeUuqh94251occSpvt11+9AXUmnT60NSCZAP1iqC9/koR6633g42JvMtB8nAkfVZWubXPQdW6azjuuuGU8rkj642o+iy8Q606K3Wr2Z9DxtPTpR1P4szvwWpFHds2JkWTkX7Xpo0ji73EmnC2oa8CApc0NdumN6doFtZ2XO75wnJwn9JFLjfD2cufmRl64nDJgsjg2MX58vrS6fxvdwlK/VI9k9/TxlQHyj0PWy67+CIJjxvzRMTlKIuOu7+wG4uei01CkDZiL/wv2XYRiGYRjmS5MwzEQ4jmCYiZjbczIMwzAMwzAMwzAMwzAMw3wyUhHK/ML/9cownZhtNTp/PvV3SUQYdv/ft+li2vjHNtAZNcpRsHv37rxwMsCAkOPEO5lOIFWS0uYh8xJJEfnqI7unXB4TyVbdO4SMG6UHHSrcr6Tz92vnBRgQcpx4p9Ohn+4mx36SYiLZHbgSz2605dR7cbn2ntlA6gJolz0dDvj+c3hMxAsD74NSGRClhwB/LpGFXb/sPjPAgJDjxDuZjvlhne4bGtMwDAT+cs2TKheKQgZSxOQuluaeF+biQ9s27aPN7wryj/80ZnhMIs2gb31MKgOi9JBTt4i7dqU4M8CAkOPEO5mOLndIOP6/9gVualSAv/PMPmkwiLooMXIXS3NFf0f4WYka3H+GxxwilRHVZbp60fU7/jMDDAg5TryT6ZyhrhznZDrJQF24v4uLOxHQICgUqssEwb8f2d+1mx+D3d/wmHOry2yd4XXtuXhmgAEhx4l3Mp0z1BVJFdBurCYajUSZHwsRVuoSNCxdrC4dfmxnsTFiOqyuqdWV9f6cUhe53QC0VBcMj0UUqZHVpZUa6JuHx0RYXVOpy+w4l3b/hLdGxzKx0VJQV0E/CxfjqgskMnDVOTwmMfO8i5qkb951KsCAkOPEO52OwE2Agp5dBLRCt5bQFjj4JmNBO/BCAtm4vivLh0pkeEzDvOpStCNSz4LtZIABIceJdzqdxGzt1LPcE2GUJrnEjahUole4n0Yk48SlfWfHU1c8WCLDYxrmVVdBL7J7bHcywICQ48Q7Ix3cMyvs20XAbI/l4oBa2F2gaEepIgYPNpq6FhJmchFtSfzXYlrmVVcq8yQNetI7GWBAyHHinZXOyZ2p7NZMOO8qN2k6c2+q8yk3e/v4bhnDY1rmVZfp3n3f854MMCDkOPHGTOfyRQXTycl91c7feG3oFm1jbe02PB1WFzMdbjD3f6RgGIZhGIZhGIZh/hI3DDMRrC5mOlhdzHSwupjpYHUx08HqYqaD1cVMB6uLmY4h6rq9G78czFekoa5/bu/Pi3O/nqgwhs3rn0Hxnu638n4zNNcBsS/L8JCTFT/fMkNtODRem8owlbo2P7dSnhd5SnX9s1tLeTsk5uN2fXe3lk/D8h0Q+7IM9zlZ8fMtM9SGw23fpjZMqa4d/Z/082JPqa7d9nk3rIbP8uXm5n37a1i+A2JfluE+Jyt+vmWG2nC47dvUhqnU9Xv363x1/bP7+fqIl3DxE8vzvttsfuK9W/N5MP/zfrMZVsOHN/z7azss3wGxL8twn5MVP98yQ2043PZtasOU6non93Ve7Pvt+vebRM/3tN0+v0mYrr3I9fbtTa5ft2+/5ZnTt2MMrKFxIq/yfVCmA2JflmEHJyt+vmWGqmQUddWGaczqz1eXvMNyvIJK1++YzAbVRVf4lxzjBQysoXzGv7uBmQ+IfVmGHXwVddWGGaQumnetQaGPj7ZEL3JHV/iu4lZeto5idQ0OMCDkOPFajKIuHF03r/f3Dw11YdlYXQNhdSG1unby7e729TrUtaU6DZ53fTz2ZRl28FXUVRvmMnWRSjfXoa71A/4duoQbEPuyDDv4KuqqDXORul5wZg9qugp1vW7fb+x4/XdiX5ZhB19FXbVhLvNd6+3tZre9DnU9yYfN0/NQ6wyIfVmGHXwVddWGuUxdL2sp17fgwa5AXTd/oDDbwV+wD4h9WYaHfBV11Ya59H/g0DuJa+GywgyIfVW1vyasYfj/dzHTwepipoPVxUwHq4uZDlYXMx2sLmY6WF3MdLC6mOmYe38nhmEYhmEYhmEY5hOQiFDm453YdQHJov+A2yliDox9WYZtUmgA0Xdm/MkAA0KOE++QPcP8MMfPLcZI+hKylZLDjqUbHnNg7Msy3EeHyveVPH665MkAA0KOE2+fA8NocFxpmsuLz0S+FC8MvGFNNjzmwNiXZbhPgIdGZ+G34QEGhBwn3j4HhnGlTPAkYtnjF/UiCDBO4mUrcwXXQbDAo/hcew/CLC464i/Nhh6pOTzmwNiXZbhPTify9hxKfTLAgJDjxNvnwDA6QVn5PaeuO2kYBkIGeD6jyoWSeHZoIEWsUJFLc88Lc9Gr0HMY3mSXNfa8Z8sal1EcHzxOBhgQcpx4HXQYJlOy50hroTLMOgV1eSBHFWbg8XwcVAWqi67ob9/R7QNLNnnMgbFHVBf2296TVU8GGBBynHgddBgm7nNdTo4ntesks9m7ED+j4HjWujl1PSeneumh8KyuYQEGhBwnXgeHhllJ2XdSdiRV4GZV9ho9WObHQoSVulBnrK5hfHF1gbi83hi6gDXlqqkuGB6LKFKsrhEIqWn75l2nAgwIOU68DvYNc1JciI5hym7UlULwgkZSweoaAZXj354F28kAA0KOE6+DPcOc9lzKs7E8et+2AJ0tUeYZ+64xKHBa22e7kwEGhBwnXgdtw2TSEhyNIMIoTXLwVp5UiV5JhVOxOHFzyeoagVTmSRr0pHcywICQ48TrYO9912l16SU8Dl2cdxVKSoGDYgG3ihg8GKvrYlwwati3rDoZYEDIceIdMsAwmaY3EDjv0rq8NfdXR18IfepbjpMBBoQcJ954jLJkZZhOWF3MdLgBj4bMefx/0Td8DxE6gQAAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "ca6ee5eb",
   "metadata": {},
   "source": [
    "When a new message comes in, our Multinomial Naive Bayes algorithm will classify it based on the results from these two equations:\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "To calculate **P(w$_i$|Spam)** and **P(w$_i$|Ham)** in the formulas above, we will use the following equations:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "Here's a summary of the terms in the equations above:\n",
    "\n",
    "\\begin{aligned}\n",
    "&N_{w_i|Spam} = \\text{the number of times the word } w_i \\text{ occurs in spam messages.} \\\\\n",
    "&N_{w_i|Ham} = \\text{the number of times the word } w_i \\text{ occurs in non-spam messages.} \\\\\n",
    "&N_{Spam} = \\text{total number of words in spam messages.} \\\\\n",
    "&N_{Ham} = \\text{total number of words in non-spam messages.} \\\\\n",
    "&N_{Vocabulary} = \\text{total number of words in the vocabulary.} \\\\\n",
    "&\\alpha = 1 \\ \\ \\ (\\alpha \\text{ is the smoothing parameter}).\n",
    "\\end{aligned}\n",
    "\n",
    "To calculate the probabilities, we first need to clean the data to put it into a format that allows us to easily extract the necessary information. Our goal is to transform the data as shown in the following table:\n",
    "\n",
    "![download.png](attachment:download.png)\n",
    "\n",
    "Regarding this transformation, we observe that:\n",
    "- The `SMS` column no longer exists. Instead, it has been replaced by a series of new columns, where each column represents a unique word from the vocabulary.\n",
    "- Each row describes a single message. For example, the first row corresponds to the message: `SECRET PRIZE! CLAIM SECRET PRIZE NOW!!`, and it has the values: `spam`, `2`, `2`, `1`, `1`, `0`, `0`, `0`, `0`, `0`. These values indicate that:\n",
    "    - The message is classified as spam.\n",
    "    - The words `claim` and `now` each occur once in the message.\n",
    "    - The words `secret` and `prize` each occur twice in the message.\n",
    "    - The words `coming`, `to`, `my`, `party`, and `winner` do not appear in the message.\n",
    "- All words in the vocabulary are in lowercase, so `SECRET` and `secret` are considered the same word.\n",
    "- Punctuation is no longer taken into account. For instance, we can't deduce from the table that the first message originally contained three exclamation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357cf93",
   "metadata": {},
   "source": [
    "### 3.2. Removing Punctuation and Converting Text to Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d197a",
   "metadata": {},
   "source": [
    "Let's start the data cleaning process by removing punctuation and converting all words to lowercase in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caca602a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Squeeeeeze!! This is christmas hug.. If u lik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>And also I've sorta blown him off a couple tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Mmm thats better now i got a roast down me! i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Mm have some kanji dont eat anything heavy ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>So there's a ring that comes with the guys cos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Squeeeeeze!! This is christmas hug.. If u lik ...\n",
       "1   ham  And also I've sorta blown him off a couple tim...\n",
       "2   ham  Mmm thats better now i got a roast down me! i...\n",
       "3   ham      Mm have some kanji dont eat anything heavy ok\n",
       "4   ham  So there's a ring that comes with the guys cos..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of the training set before any changes\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af811266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>squeeeeeze   this is christmas hug   if u lik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>and also i ve sorta blown him off a couple tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>mmm thats better now i got a roast down me  i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>mm have some kanji dont eat anything heavy ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>so there s a ring that comes with the guys cos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  squeeeeeze   this is christmas hug   if u lik ...\n",
       "1   ham  and also i ve sorta blown him off a couple tim...\n",
       "2   ham  mmm thats better now i got a roast down me  i ...\n",
       "3   ham      mm have some kanji dont eat anything heavy ok\n",
       "4   ham  so there s a ring that comes with the guys cos..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation and replace it with spaces, then convert all text to lowercase\n",
    "training_set['SMS'] = training_set['SMS'].str.replace(r\"\\W\", \" \", regex=True)\n",
    "training_set['SMS'] = training_set['SMS'].str.lower()\n",
    "\n",
    "# Display the first 5 rows of the training set after the changes\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38edb869",
   "metadata": {},
   "source": [
    "As we can see, words like `Squeeeeeze!!` are now `squeeeeeze`, and contractions such as `I've` have been split into `i ve`. This ensures consistency for further analysis, though splitting contractions and removing punctuation could slightly alter the original meaning of some texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf11290",
   "metadata": {},
   "source": [
    "### 3.3. Creating the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adeec7e",
   "metadata": {},
   "source": [
    "With the exception of the `Label` column, every other new column after performing our data transformation should represent a unique word in our vocabulary. More specifically, each column should indicate the frequency of that unique word for any given message. Note that we refer to the set of unique words as the vocabulary.\n",
    "\n",
    "Now, let's create a list of all the unique words that occur in the messages of our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155496db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7816"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the 'SMS' column into lists of words for each message\n",
    "training_set['SMS'] = training_set['SMS'].str.split()\n",
    "vocabulary = []\n",
    "\n",
    "# Iterate through each message and add each word to the vocabulary list\n",
    "for sms in training_set['SMS']:\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "# Convert the vocabulary to a set and display the total number of unique words in the vocabulary\n",
    "vocabulary = list(set(vocabulary))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e939290",
   "metadata": {},
   "source": [
    "As we can see, there are `7,816` unique words in all the messages of our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b22db9",
   "metadata": {},
   "source": [
    "### 3.4. Updating the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c3aff",
   "metadata": {},
   "source": [
    "We will now use the vocabulary to perform the desired transformation. To do this, we'll first initialize a dictionary where each key is a unique word from the vocabulary list, and each value will be a list of zeros with the same length as the number of rows in the `SMS` column.\n",
    "\n",
    "After counting the occurrences of each word in every message and updating the word counts dictionary, we'll convert it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc77ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket</th>\n",
       "      <th>hopeso</th>\n",
       "      <th>sorting</th>\n",
       "      <th>chest</th>\n",
       "      <th>chiong</th>\n",
       "      <th>click</th>\n",
       "      <th>paying</th>\n",
       "      <th>aiyar</th>\n",
       "      <th>spjanuary</th>\n",
       "      <th>cheer</th>\n",
       "      <th>...</th>\n",
       "      <th>billy</th>\n",
       "      <th>tihs</th>\n",
       "      <th>mint</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>nightnight</th>\n",
       "      <th>slippers</th>\n",
       "      <th>blood</th>\n",
       "      <th>oz</th>\n",
       "      <th>close</th>\n",
       "      <th>77</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7816 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket  hopeso  sorting  chest  chiong  click  paying  aiyar  spjanuary  \\\n",
       "0       0       0        0      0       0      0       0      0          0   \n",
       "1       0       0        0      0       0      0       0      0          0   \n",
       "2       0       0        0      0       0      0       0      0          0   \n",
       "3       0       0        0      0       0      0       0      0          0   \n",
       "4       0       0        0      0       0      0       0      0          0   \n",
       "\n",
       "   cheer  ...  billy  tihs  mint  aaooooright  nightnight  slippers  blood  \\\n",
       "0      0  ...      0     0     0            0           0         0      0   \n",
       "1      0  ...      0     0     0            0           0         0      0   \n",
       "2      0  ...      0     0     0            0           0         0      0   \n",
       "3      0  ...      0     0     0            0           0         0      0   \n",
       "4      0  ...      0     0     0            0           0         0      0   \n",
       "\n",
       "   oz  close  77  \n",
       "0   0      0   0  \n",
       "1   0      0   0  \n",
       "2   0      0   0  \n",
       "3   0      0   0  \n",
       "4   0      0   0  \n",
       "\n",
       "[5 rows x 7816 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to hold counts of each unique word for each message in the training set\n",
    "word_counts = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "# Iterate through each message, counting occurrences of each word and updating the dictionary\n",
    "for i, sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts[word][i] += 1\n",
    "\n",
    "# Convert the word counts dictionary into a DataFrame and display the first few rows\n",
    "word_counts = pd.DataFrame(word_counts)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfe631",
   "metadata": {},
   "source": [
    "The word counts DataFrame contains `7,816` columns, with each column representing a unique word from the vocabulary and the values indicating the frequency of each word in the corresponding SMS messages. Next, we're going to concatenate the word counts DataFrame with the original training set to include word frequency data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328db4d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>ticket</th>\n",
       "      <th>hopeso</th>\n",
       "      <th>sorting</th>\n",
       "      <th>chest</th>\n",
       "      <th>chiong</th>\n",
       "      <th>click</th>\n",
       "      <th>paying</th>\n",
       "      <th>aiyar</th>\n",
       "      <th>...</th>\n",
       "      <th>billy</th>\n",
       "      <th>tihs</th>\n",
       "      <th>mint</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>nightnight</th>\n",
       "      <th>slippers</th>\n",
       "      <th>blood</th>\n",
       "      <th>oz</th>\n",
       "      <th>close</th>\n",
       "      <th>77</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[squeeeeeze, this, is, christmas, hug, if, u, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[and, also, i, ve, sorta, blown, him, off, a, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[mmm, thats, better, now, i, got, a, roast, do...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7818 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  ticket  hopeso  \\\n",
       "0   ham  [squeeeeeze, this, is, christmas, hug, if, u, ...       0       0   \n",
       "1   ham  [and, also, i, ve, sorta, blown, him, off, a, ...       0       0   \n",
       "2   ham  [mmm, thats, better, now, i, got, a, roast, do...       0       0   \n",
       "\n",
       "   sorting  chest  chiong  click  paying  aiyar  ...  billy  tihs  mint  \\\n",
       "0        0      0       0      0       0      0  ...      0     0     0   \n",
       "1        0      0       0      0       0      0  ...      0     0     0   \n",
       "2        0      0       0      0       0      0  ...      0     0     0   \n",
       "\n",
       "   aaooooright  nightnight  slippers  blood  oz  close  77  \n",
       "0            0           0         0      0   0      0   0  \n",
       "1            0           0         0      0   0      0   0  \n",
       "2            0           0         0      0   0      0   0  \n",
       "\n",
       "[3 rows x 7818 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the word counts DataFrame with the original training set\n",
    "training_set = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4ba9c",
   "metadata": {},
   "source": [
    "The updated training set consists of `7,818` columns, with the first two columns representing the label and the processed SMS message as a list of words. The additional columns reflect the frequency of `7,816` unique words from the vocabulary in each message. Entries that remain `0` indicate the absence of those words in the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4cd91",
   "metadata": {},
   "source": [
    "## 4. Calculating Constants and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4177e00",
   "metadata": {},
   "source": [
    "Now that we have completed our training set, we can begin creating the spam filter. To start, let's calculate **P(Spam)**, **P(Ham)**, **N<sub>Vocabulary</sub>**, **N<sub>Spam</sub>**, and **N<sub>Ham</sub>**. We will also use **Laplace smoothing** and set `α = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a60f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique words and set the smoothing parameter (α)\n",
    "n_vocabulary = len(vocabulary)\n",
    "alpha = 1\n",
    "\n",
    "# Separate the training set into spam and ham messages\n",
    "spams = training_set[training_set['Label'] == 'spam']\n",
    "hams = training_set[training_set['Label'] == 'ham']\n",
    "\n",
    "# Calculate the probabilities of spam and ham\n",
    "p_spam = len(spams) / len(training_set)\n",
    "p_ham = len(hams) / len(training_set)\n",
    "\n",
    "# Count the total number of words in spam messages\n",
    "n_words_per_spam = spams['SMS'].apply(len)\n",
    "n_spam = n_words_per_spam.sum()\n",
    "\n",
    "# Count the total number of words in ham messages\n",
    "n_words_per_ham = hams['SMS'].apply(len)\n",
    "n_ham = n_words_per_ham.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769fc37",
   "metadata": {},
   "source": [
    "The terms **P(Spam)**, **P(Ham)**, **N<sub>Vocabulary</sub>**, **N<sub>Spam</sub>**, **N<sub>Ham</sub>** will have constant values in our equations for every new message, regardless of the individual words in it. However, **P(w$_i$|Spam)** and **P(w$_i$|Ham)** will vary depending on the individual words. For instance, **P(\"secret\"|Spam)** will have a specific probability value, while **P(\"cousin\"|Spam)** or **P(\"lovely\"|Spam)** will most likely have different values.\n",
    "\n",
    "Although both **P(w$_i$|Spam)** and **P(w$_i$|Ham)** vary depending on the word, the probability for each individual word is constant for every new message. For instance, let's say we receive two new messages: `secret code` and `secret party 2night`. We'll need to calculate **P(\"secret\"|Spam)** for both messages, and we can use the training set to obtain the values we need to find a result for the following equation:\n",
    "\\begin{equation}\n",
    "P(\\text{\"secret\"}|Spam) = \\frac{N_{\"secret\"|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "The steps we take to calculate **P(\"secret\"|Spam)** will be identical for both of our new messages above, or for any other new message that contains the word `secret`. The key detail here is that calculating **P(\"secret\"|Spam)** only depends on the training set, and as long as we don't make changes to the training set, **P(\"secret\"|Spam)** stays constant. The same reasoning also applies to **P(\"secret\"|Ham)**.\n",
    "\n",
    "This means we can use our training set to calculate the probability for each word in our vocabulary. For instance, if our vocabulary contained only the words `lost`, `navigate`, and `sea`, we'd need to calculate six probabilities:\n",
    "- **P(\"lost\"|Spam)** and **P(\"lost\"|Ham)**\n",
    "- **P(\"navigate\"|Spam)** and **P(\"navigate\"|Ham)**\n",
    "- **P(\"sea\"|Spam)** and **P(\"sea\"|Ham)**\n",
    "\n",
    "Since we have `7,783` words in our vocabulary, we'll need to calculate a total of `15,566` probabilities. For each word, we need to calculate both **P(w$_i$|Spam)** and **P(w$_i$|Ham)**. In more technical terms, these probability values are referred to as **parameters**.\n",
    "\n",
    "The fact that we calculate so many values before even starting the classification of new messages makes the Naive Bayes algorithm very fast, especially compared to other algorithms. When a new message comes in, most of the necessary computations are already done, allowing the algorithm to classify the message almost instantly.\n",
    "\n",
    "If we don’t calculate all these values beforehand, the algorithm will need to perform these calculations every time a new message comes in. Imagine the algorithm is used to classify `1,000,000` new messages; why repeat the same calculations a million times when we could simply do them once at the start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386fd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store probabilities for each word in spam and ham messages\n",
    "parameters_spam = {unique_word: 0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word: 0 for unique_word in vocabulary}\n",
    "\n",
    "# Loop through each word in the vocabulary\n",
    "for word in vocabulary:\n",
    "    \n",
    "    # Calculate the probability of the word given it's in a spam message\n",
    "    n_word_given_spam = spams[word].sum()\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "    \n",
    "    # Calculate the probability of the word given it's in a ham message\n",
    "    n_word_given_ham = hams[word].sum()\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a7672",
   "metadata": {},
   "source": [
    "## 5. Building and Testing the Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4007243",
   "metadata": {},
   "source": [
    "Now that we've calculated all the necessary constants and parameters, we can begin building the spam filter. This filter functions as follows:\n",
    "\n",
    "- It takes a new message **(w1, w2, ..., wn)** as input.\n",
    "- It calculates **P(Spam|w1, w2, ..., wn)** and **P(Ham|w1, w2, ..., wn)**.\n",
    "- It compares these values and:\n",
    "    - If **P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn)**, the message is classified as ham.\n",
    "    - If **P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn)**, the message is classified as spam.\n",
    "    - If **P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn)**, the algorithm flags the message for human review.\n",
    "    \n",
    "Note that some new messages may include words that aren't part of the vocabulary. However, we can safely ignore these words when calculating the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f87faed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the spam filter function\n",
    "def spam_filter(message):\n",
    "\n",
    "    # Remove punctuation, convert to lowercase, and split the message into words\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    # Initialize probabilities using prior values\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Calculate probabilities based on message content\n",
    "    for word in message:\n",
    "        \n",
    "        # Multiply by the word's probability given it's spam\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "            \n",
    "        # Multiply by the word's probability given it's ham\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    # Display the calculated probabilities\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "    \n",
    "    # Classify the message based on the probabilities\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('The probabilities are equal! Please ask a person to classify this.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24894b",
   "metadata": {},
   "source": [
    "Next, let's test the `spam_filter` function using two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0303f97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.5223001843661562e-25\n",
      "P(Ham|message): 1.2176099861344542e-27\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "# Call the function with a sample message\n",
    "spam_filter('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ee4d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 4.74693224259436e-25\n",
      "P(Ham|message): 2.7878169823581606e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "# Call the function with a sample message\n",
    "spam_filter(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36988bd",
   "metadata": {},
   "source": [
    "## 6. Measuring the Spam Filter's Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c672f62d",
   "metadata": {},
   "source": [
    "Finally, we've successfully created a spam filter and classified two new messages. Now, we'll test how well the spam filter performs on our test set of `1,114` messages.\n",
    "\n",
    "The algorithm will output a classification label for each message in our test set, which we'll then compare to the actual label provided by a human. Since the algorithm hasn't seen these `1,114` messages during training, each message in the test set is essentially new from the algorithm's perspective.\n",
    "\n",
    "To begin, let's apply a slightly modified version of the `spam_filter` function to each SMS message in the `SMS` column of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53435463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Was playng 9 doors game and gt racing on phone...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>I dont thnk its a wrong calling between us</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>All e best 4 ur exam later.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hey what how about your project. Started aha da.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Dunno, my dad said he coming home 2 bring us o...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS Predicted\n",
       "0   ham  Was playng 9 doors game and gt racing on phone...       ham\n",
       "1   ham         I dont thnk its a wrong calling between us       ham\n",
       "2   ham                        All e best 4 ur exam later.       ham\n",
       "3   ham   Hey what how about your project. Started aha da.       ham\n",
       "4   ham  Dunno, my dad said he coming home 2 bring us o...       ham"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the modified spam filter function\n",
    "def spam_filter_modified(message):\n",
    "\n",
    "    # Remove punctuation, convert to lowercase, and split the message into words\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    # Initialize probabilities using prior values\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Calculate probabilities based on message content\n",
    "    for word in message:\n",
    "        \n",
    "        # Multiply by the word's probability given it's spam\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "            \n",
    "        # Multiply by the word's probability given it's ham\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "    \n",
    "    # Classify the message based on the probabilities\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'The probabilities are equal! Please ask a person to classify this.'\n",
    "\n",
    "# Apply the modified spam filter to the test set and store the predicted labels\n",
    "test_set['Predicted'] = test_set['SMS'].apply(spam_filter_modified)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c1000",
   "metadata": {},
   "source": [
    "The model correctly predicts all the messages in the first five rows of the test set as `ham`, matching the true labels. However, further analysis is needed for overall test accuracy.\n",
    "\n",
    "We'll compare the predicted values with the actual values to assess how well our spam filter classifies new messages. To evaluate this, we will use accuracy as our metric:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correctly classified messages}}{\\text{Total number of classified messages}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4492995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classifications: 1092\n",
      "Incorrect classifications: 22\n",
      "Filter's accuracy: 0.9802513464991023\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for correct predictions and total number of messages\n",
    "correct = 0\n",
    "total = test_set.shape[0]\n",
    "\n",
    "# Iterate through each row in the test set\n",
    "for row in test_set.iterrows():\n",
    "    \n",
    "    # Access the actual row data, and check if the predicted label matches the actual label\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['Predicted']:\n",
    "        correct += 1\n",
    "\n",
    "# Print the number of correct and incorrect classifications, as well as the accuracy\n",
    "print(\"Correct classifications:\", correct)\n",
    "print(\"Incorrect classifications:\", total - correct)\n",
    "print(\"Filter's accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f2bea",
   "metadata": {},
   "source": [
    "The spam filter correctly classified `1,092` out of `1,114` messages, resulting in an accuracy of `98.02%`. Only `22` messages were misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2b010",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7fd065",
   "metadata": {},
   "source": [
    "In this project, we built a spam filter for SMS messages using the Multinomial Naive Bayes algorithm. The dataset, containing `5,572` manually classified SMS messages, was sourced from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/228/sms+spam+collection). Initially, the dataset had two columns: the label (`spam` or `ham`) and the message content. Approximately `86.59%` of the messages were labeled as ham, and `13.41%` as spam. This reflects a realistic distribution, as most real-world messages are non-spam.\n",
    "\n",
    "To assess the performance of our spam filter on new messages, we split the dataset into `80%` for training and `20%` for testing. Out of `5,572` messages, the training set contains `4,458` messages, and the test set includes `1,114`. Moreover, the percentage of spam and ham messages in both sets mirrors the overall dataset, with about `87%` of the messages labeled as ham and `13%` as spam.\n",
    "\n",
    "After explaining the Multinomial Naive Bayes classification algorithm in detail, we cleaned the training data by removing punctuation and converting all words to lowercase. The data cleaning process ensured consistency for analysis, though splitting contractions and removing punctuation may have slightly altered the original meaning of some messages.\n",
    "\n",
    "After updating the training set, it now contains `7,818` columns. The first two columns represent the label and the processed SMS message as a list of words, while the additional columns show the frequency of `7,816` unique words from the vocabulary for each message. Entries with a value of `0` indicate that those words are absent in the respective messages.\n",
    "\n",
    "After calculating all the necessary constants and parameters, we proceeded to build the spam filter and tested it on two sample messages. We then evaluated its performance on our test set of `1,114` messages. The algorithm classified each message, and we compared its predictions to the actual human-provided labels. Since the algorithm hadn't encountered these messages during training, each was essentially new.\n",
    "\n",
    "The spam filter correctly classified `1,092` out of `1,114` messages, achieving an accuracy of `98.02%`, with only `22` misclassified messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
