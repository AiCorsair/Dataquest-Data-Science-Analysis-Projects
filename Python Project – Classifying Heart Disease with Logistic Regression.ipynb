{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d7ce14",
   "metadata": {},
   "source": [
    "# Classifying Heart Disease Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb644fe0",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b4ed73",
   "metadata": {},
   "source": [
    "In this project, we'll be exploring the [Heart Disease](https://archive.ics.uci.edu/dataset/45/heart+disease) dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/). This dataset originates from the Cleveland Clinic Foundation, which recorded various patient characteristics, including age and chest pain, to classify the presence of heart disease in individuals. This is a prime example of how we can use machine learning to solve problems that have a real impact on people's lives.\n",
    "\n",
    "We'll walk through the machine learning pipeline, starting from examining the dataset itself to creating a logistic regression model. Specifically, we'll use a dataset that [Dataquest](https://www.dataquest.io/) partially cleaned to facilitate binary classification. The original dataset, which contains multiple classes, can be downloaded from the original website. Moreover, the target variable in the original dataset is named `num`.\n",
    "\n",
    "To get started, let's import the relevant libraries and the dataset we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309f71a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  \\\n",
       "0           1   63    1   1       145   233    1        2      150      0   \n",
       "1           2   67    1   4       160   286    0        2      108      1   \n",
       "2           3   67    1   4       120   229    0        2      129      1   \n",
       "3           4   37    1   3       130   250    0        0      187      0   \n",
       "4           5   41    0   2       130   204    0        2      172      0   \n",
       "\n",
       "   oldpeak  slope   ca thal  present  \n",
       "0      2.3      3  0.0  6.0        0  \n",
       "1      1.5      2  3.0  3.0        1  \n",
       "2      2.6      2  2.0  7.0        1  \n",
       "3      3.5      3  0.0  3.0        0  \n",
       "4      1.4      1  0.0  3.0        0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the relevant libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the heart disease dataset and display the first few rows\n",
    "heart = pd.read_csv(\"Datasets/heart_disease.csv\")\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d86b1d",
   "metadata": {},
   "source": [
    "The first few rows show patients with varying levels of chest pain, blood pressure, and cholesterol. Fasting blood sugar and exercise-induced angina differ across patients. The target variable, `present`, indicates whether heart disease is diagnosed. In addition, the features include measurements like maximum heart rate, ST depression, and the number of major vessels, which vary across individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5b3fd",
   "metadata": {},
   "source": [
    "## 2. Exploring and Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f723de8",
   "metadata": {},
   "source": [
    "Before we build our classification model, we should further explore the dataset and make any necessary adjustments. This may include converting categorical variables into dummy variables or scaling features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24387cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 15 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  303 non-null    int64  \n",
      " 1   age         303 non-null    int64  \n",
      " 2   sex         303 non-null    int64  \n",
      " 3   cp          303 non-null    int64  \n",
      " 4   trestbps    303 non-null    int64  \n",
      " 5   chol        303 non-null    int64  \n",
      " 6   fbs         303 non-null    int64  \n",
      " 7   restecg     303 non-null    int64  \n",
      " 8   thalach     303 non-null    int64  \n",
      " 9   exang       303 non-null    int64  \n",
      " 10  oldpeak     303 non-null    float64\n",
      " 11  slope       303 non-null    int64  \n",
      " 12  ca          303 non-null    object \n",
      " 13  thal        303 non-null    object \n",
      " 14  present     303 non-null    int64  \n",
      "dtypes: float64(1), int64(12), object(2)\n",
      "memory usage: 35.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display summary information about the dataset\n",
    "heart.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745586b",
   "metadata": {},
   "source": [
    "We can consult the official [page](https://archive.ics.uci.edu/dataset/45/heart+disease) to learn more about the column names, most of which include descriptions.\n",
    "\n",
    "The dataset contains `303` rows and `15` columns. The `present` column is our binary outcome of interest, where `0` indicates the absence of heart disease, and `1` indicates its presence. Surprisingly, the columns `ca` and `thal` are of object data type instead of numerical, so let's examine them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89355cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thal\n",
       "3.0    166\n",
       "7.0    117\n",
       "6.0     18\n",
       "?        2\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ca\n",
       "0.0    176\n",
       "1.0     65\n",
       "2.0     38\n",
       "3.0     20\n",
       "?        4\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the count of each unique value in 'thal' and 'ca' columns\n",
    "display(heart['thal'].value_counts())\n",
    "display(heart['ca'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f24edf0",
   "metadata": {},
   "source": [
    "The `?` values appear to indicate missing or unknown data. Since `?` in the `thal` and `ca` columns occurred `2` and `4` times, respectively, we can replace `?` with the most common value in each column.\n",
    "\n",
    "Since the `Unnamed: 0` column appears to be an index column, it doesn't contain any meaningful information for analysis, so it should be removed to avoid potential confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bfe38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '?' in the 'thal' column with the most common value and convert to float\n",
    "heart['thal'] = heart['thal'].replace('?', heart['thal'].mode()[0])\n",
    "heart['thal'] = heart['thal'].astype(float)\n",
    "\n",
    "# Replace '?' in the 'ca' column with the most common value and convert to float\n",
    "heart['ca'] = heart['ca'].replace('?', heart['ca'].mode()[0])\n",
    "heart['ca'] = heart['ca'].astype(float)\n",
    "\n",
    "# Remove the index column 'Unnamed: 0'\n",
    "heart.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74442ce3",
   "metadata": {},
   "source": [
    "Next, let's compute the proportion of each class in the target variable `present` to check for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d833e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "present\n",
       "0    0.541254\n",
       "1    0.458746\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the proportion of each class in the target variable 'present'\n",
    "heart['present'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2bdfd",
   "metadata": {},
   "source": [
    "The dataset is slightly imbalanced, with `54.12%` of the instances indicating no heart disease and `45.87%` indicating the presence of heart disease. \n",
    "\n",
    "Next, we'll check the mean of the predictors based on the outcome, as they could be informative for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a674650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>present</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.59</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.79</td>\n",
       "      <td>129.25</td>\n",
       "      <td>242.64</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.84</td>\n",
       "      <td>158.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.63</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3.59</td>\n",
       "      <td>134.57</td>\n",
       "      <td>251.47</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.17</td>\n",
       "      <td>139.26</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.13</td>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           age   sex    cp  trestbps    chol   fbs  restecg  thalach  exang  \\\n",
       "present                                                                       \n",
       "0        52.59  0.56  2.79    129.25  242.64  0.14     0.84   158.38   0.14   \n",
       "1        56.63  0.82  3.59    134.57  251.47  0.16     1.17   139.26   0.55   \n",
       "\n",
       "         oldpeak  slope    ca  thal  \n",
       "present                              \n",
       "0           0.59   1.41  0.27  3.79  \n",
       "1           1.57   1.83  1.13  5.82  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by 'present' and calculate the mean for each column, rounded to 2 decimal places\n",
    "by_disease_status = heart.groupby(heart['present']).mean()\n",
    "by_disease_status.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83d52d",
   "metadata": {},
   "source": [
    "As we can see, there are noticeable differences in the mean of each column based on the outcome, with all columns except `thalach` showing a larger mean in the presence of heart disease. Therefore, we will keep all features to create our logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6a2aa",
   "metadata": {},
   "source": [
    "## 3. Dividing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323f397",
   "metadata": {},
   "source": [
    "Now that we have selected our predictors, we need to set aside data for final model assessment. We‚Äôll need the following:\n",
    "1. A training set to estimate the logistic regression coefficients.\n",
    "2. A test set to evaluate the model‚Äôs predictive ability.\n",
    "\n",
    "The model will be trained on the training set, and its predictive performance will be assessed on the test set. Additionally, we need to ensure that both sets contain both **cases** and **non-cases**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58af2087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train (no disease): 115\n",
      "Y_train (disease): 97\n",
      "Y_test (no disease): 49\n",
      "Y_test (disease): 42\n"
     ]
    }
   ],
   "source": [
    "# Select predictors and target variable\n",
    "X = heart.drop([\"present\"], axis=1)\n",
    "y = heart[\"present\"]\n",
    "\n",
    "# Split the data into training and test sets (70% training, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)\n",
    "\n",
    "# Initialize StandardScaler and transform the training and test sets\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Print the number of cases and non-cases in the training set\n",
    "print(\"Y_train (no disease):\", sum(y_train == 0))\n",
    "print(\"Y_train (disease):\", sum(y_train == 1))\n",
    "\n",
    "# Print the number of cases and non-cases in the test set\n",
    "print(\"Y_test (no disease):\", sum(y_test == 0))\n",
    "print(\"Y_test (disease):\", sum(y_test == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d602e",
   "metadata": {},
   "source": [
    "The distribution of cases and non-cases is fairly balanced in both the training and test sets, with slightly more non-disease cases than disease cases in each. This suggests that the model will have a reasonable opportunity to learn from both classes during training and be evaluated on them in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a30a0",
   "metadata": {},
   "source": [
    "## 4. Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386047a1",
   "metadata": {},
   "source": [
    "With the heart disease dataset divided, let‚Äôs build the classification model and conduct some initial assessments. Here are some guiding questions to consider:\n",
    "- What is the training accuracy, sensitivity, and specificity?\n",
    "- Does the model perform better on cases or non-cases, or does it perform equally well?\n",
    "\n",
    "Typically, training metrics are optimistic estimations of the model's performance, so we should expect slightly poorer metrics when the model is applied to new data. If the training metrics are too high, it might indicate that our model is overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61739d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8585\n",
      "Training Sensitivity: 0.8041\n",
      "Training Specificity: 0.9043\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the training accuracy, and make predictions on the training data\n",
    "accuracy = model.score(X_train, y_train)\n",
    "predictions = model.predict(X_train)\n",
    "\n",
    "# Calculate true positives, false positives, true negatives, and false negatives\n",
    "tp = sum((predictions == 1) & (y_train == 1))\n",
    "fp = sum((predictions == 1) & (y_train == 0))\n",
    "tn = sum((predictions == 0) & (y_train == 0))\n",
    "fn = sum((predictions == 0) & (y_train == 1))\n",
    "\n",
    "# Compute sensitivity and specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print training metrics: accuracy, sensitivity, and specificity\n",
    "print(\"Training Accuracy:\", round(accuracy, 4))\n",
    "print(\"Training Sensitivity:\", round(sensitivity, 4))\n",
    "print(\"Training Specificity:\", round(specificity, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa1a12",
   "metadata": {},
   "source": [
    "The model shows a strong training accuracy of `85.85%`, with higher specificity (`90.43%`) compared to sensitivity (`80.41%`). This indicates that the model is better at correctly identifying non-disease cases (`specificity`) than disease cases (`sensitivity`). The lower sensitivity suggests that the model might miss some cases of heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f674661",
   "metadata": {},
   "source": [
    "## 5. Interpreting the Model Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a25c6",
   "metadata": {},
   "source": [
    "Now that we've created our model, let's examine the coefficients to see if they make sense given the problem. Logistic regression relates the binary outcome to the linear combination of predictors via the link function: \n",
    "\n",
    "$$\\log\\left(\\frac{E[Y]}{1 - E[Y]}\\right) = \\beta_0 + \\beta_1 X$$\n",
    "\n",
    "The predictors affect the outcome on the log-odds scale, and the non-intercept coefficients represent the log-odds ratio for a unit increase in a predictor: \n",
    "$$\\log\\left(\\frac{O_1}{O_0}\\right) = \\beta_1$$\n",
    "\n",
    "`ùë∂‚ÇÄ` represents the odds ratio when the predictor is `0`, and `ùë∂‚ÇÅ` represents the odds ratio when the predictor is `1`. Usually, we're interested in examining these effects on the odds scale, so we exponentiate both sides to get the following: \n",
    "\n",
    "$$\\boldsymbol{O_1 = e^{\\beta_1} O_0}$$\n",
    "\n",
    "Let‚Äôs examine what our predictors suggest about their relationship with heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5143f3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for each predictor:\n",
      "age : -0.06\n",
      "sex : 0.7\n",
      "cp : 0.48\n",
      "trestbps : 0.24\n",
      "chol : 0.23\n",
      "fbs : -0.12\n",
      "restecg : 0.16\n",
      "thalach : -0.44\n",
      "exang : 0.51\n",
      "oldpeak : 0.44\n",
      "slope : 0.2\n",
      "ca : 0.97\n",
      "thal : 0.43\n",
      "\n",
      "Exponentiated coefficients for each predictor:\n",
      "age : 0.94\n",
      "sex : 2.02\n",
      "cp : 1.62\n",
      "trestbps : 1.27\n",
      "chol : 1.25\n",
      "fbs : 0.89\n",
      "restecg : 1.18\n",
      "thalach : 0.64\n",
      "exang : 1.66\n",
      "oldpeak : 1.55\n",
      "slope : 1.22\n",
      "ca : 2.65\n",
      "thal : 1.54\n"
     ]
    }
   ],
   "source": [
    "# Define the list of predictor names\n",
    "predictors = heart.columns\n",
    "\n",
    "# Print the coefficients for each predictor\n",
    "print(\"Coefficients for each predictor:\")\n",
    "for pred, coef in zip(predictors, model.coef_[0]):\n",
    "    print(pred, \":\", round(coef, 2))\n",
    "\n",
    "print()\n",
    "\n",
    "# Print the exponentiated coefficients (odds ratios) for each predictor\n",
    "print(\"Exponentiated coefficients for each predictor:\")\n",
    "for pred, coef in zip(predictors, model.coef_[0]):\n",
    "    print(pred, \":\", round(np.exp(coef), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc383e",
   "metadata": {},
   "source": [
    "The coefficients suggest the following insights while holding the other predictors constant:\n",
    "- `ca` and `sex` have the highest positive coefficients, indicating that more vessels colored and male sex are strongly associated with increased odds of heart disease.\n",
    "- Most other predictors (`cp`, `trestbps`, `chol`, `restecg`, `exang`, `oldpeak`, `slope`, and `thal`) also show positive coefficients, with varying strengths, indicating a general association with higher odds of heart disease.\n",
    "- `age`, `fbs`, and `thalach` have negative coefficients, suggesting that increased age, fasting blood sugar, or maximum heart rate are associated with decreased odds of heart disease.\n",
    "\n",
    "The exponentiated coefficients show that each unit increase in these predictors changes the odds of heart disease by the given multiplier. For example, a one-unit increase in the number of vessels colored (`ca`) increases the odds of heart disease by about `2.65` times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0d4fa",
   "metadata": {},
   "source": [
    "## 6. Final Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c380f",
   "metadata": {},
   "source": [
    "Finally, we can assess the predictive ability of our logistic regression model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f45c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8681\n",
      "Test Sensitivity: 0.881\n",
      "Test Specificity: 0.8571\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the model on the test set and make predictions\n",
    "accuracy = model.score(X_test, y_test)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate true positives, false positives, true negatives, and false negatives\n",
    "tp = sum((predictions == 1) & (y_test == 1))\n",
    "fp = sum((predictions == 1) & (y_test == 0))\n",
    "tn = sum((predictions == 0) & (y_test == 0))\n",
    "fn = sum((predictions == 0) & (y_test == 1))\n",
    "\n",
    "# Compute sensitivity and specificity based on the test set\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print test metrics: accuracy, sensitivity, and specificity\n",
    "print(\"Test Accuracy:\", round(accuracy, 4))\n",
    "print(\"Test Sensitivity:\", round(sensitivity, 4))\n",
    "print(\"Test Specificity:\", round(specificity, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f623c58",
   "metadata": {},
   "source": [
    "The test metrics show that the model performs slightly better on the test set than on the training set. Specifically, test accuracy increased to `86.81%` from `85.85%` in training, indicating a slight improvement in overall predictive performance. However, specificity decreased from `90.43%` to `85.71%`, suggesting the model is less effective at identifying non-cases of heart disease in the test set. Conversely, sensitivity rose significantly from `80.41%` to `88.1%`, showing a major improvement in correctly identifying cases. Overall, these results suggest that the model generalizes well to new data without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2129a",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1de9d4",
   "metadata": {},
   "source": [
    "In this project, we explored a dataset originating from the Cleveland Clinic Foundation, which recorded various patient characteristics to classify the presence of heart disease. Specifically, we used a dataset that [Dataquest](https://www.dataquest.io/) partially cleaned to facilitate binary classification, with the target variable changed from `num` (in the original dataset) to `present`.\n",
    "\n",
    "The dataset initially contained `303` rows and `15` columns, with the `present` column serving as our binary outcome of interest, where `0` and `1` indicate the absence and presence of heart disease, respectively. Additionally, we replaced the `?` values in the `thal` and `ca` columns, which indicate missing or unknown data, with the most common value in each column. We also removed the `Unnamed: 0` column, as it appeared to be an index column and didn't contain any meaningful information for analysis.\n",
    "\n",
    "The dataset is slightly imbalanced, with `54.12%` of the instances indicating no heart disease and `45.87%` indicating the presence of heart disease. After examining the mean of each column based on the outcome, we noticed that all columns except `thalach` show a larger mean in the presence of heart disease. Also, we decided to keep all features to create our logistic regression model. Next, we split the data into a training set to estimate the coefficients and a test set to evaluate the model‚Äôs predictive ability.\n",
    "\n",
    "With the dataset divided, we built our logistic regression model and conducted some initial assessments. The classification model achieved a strong training accuracy of `85.85%`, with higher specificity (`90.43%`) compared to sensitivity (`80.41%`). This indicates that the model is better at correctly identifying non-disease cases (`specificity`) than disease cases (`sensitivity`). Additionally, the lower sensitivity suggests that the model might miss some cases of heart disease.\n",
    "\n",
    "The coefficients suggest the following while holding the other predictors constant: `ca` and `sex` have the highest positive coefficients, indicating that more vessels colored and male sex are strongly associated with increased odds of heart disease. Most other predictors also show positive coefficients, with varying strengths, indicating a general association with higher odds of heart disease. The columns `age`, `fbs`, and `thalach` have negative coefficients, suggesting that increased age, fasting blood sugar, or maximum heart rate are associated with decreased odds of heart disease. Furthermore, the exponentiated coefficients show that each unit increase in these predictors changes the odds of heart disease by the given multiplier. For example, a one-unit increase in the number of vessels colored (`ca`) increases the odds of heart disease by about `2.65` times.\n",
    "\n",
    "Finally, we found that the test metrics show that the model performs slightly better on the test set than on the training set. Specifically, test accuracy increased to `86.81%` from `85.85%` in training, indicating a slight improvement in overall predictive performance. However, specificity decreased from `90.43%` to `85.71%`, suggesting the model is less effective at identifying non-cases of heart disease in the test set. Conversely, sensitivity rose significantly from `80.41%` to `88.1%`, showing a major improvement in correctly identifying cases. Overall, these results suggest that the model generalizes well to new data without overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
